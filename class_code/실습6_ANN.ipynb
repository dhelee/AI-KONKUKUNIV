{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습6_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZnDI8azinw",
        "outputId": "344814b1-d3b8-4aeb-c3be-3a2b374d5c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwlC76Kp21Qz",
        "outputId": "bc14b022-f7c9-422b-d0f9-e76b4b679bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "import os\n",
        "root_dir = '/content/drive/My Drive/인공지능/week9'\n",
        "Image(os.path.join(root_dir, \"MNIST.PNG\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "/content/drive/My Drive/인공지능/week9/MNIST.PNG",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Yj5-JyNCLs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MNIST(nn.Module): # 상속 \n",
        "  def __init__(self, config):\n",
        "    super(MNIST, self).__init__()\n",
        "\n",
        "    # 데이터의 width 길이를 저장하는 parameter\n",
        "    self.width = config[\"input_width_size\"]\n",
        "    # 데이터의 height 길이\n",
        "    self.height = config[\"input_height_size\"]\n",
        "\n",
        "    # 과제에서 사용되는 파라미터\n",
        "    self.feature_size = config[\"feature_size\"]\n",
        "\n",
        "    # 분류해야 하는 label 수\n",
        "    self.num_labels = config[\"num_labels\"]\n",
        "\n",
        "    # 활성화 함수로 Sigmoid 사용\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    # Neural Network\n",
        "    # 입력 feature는 28*28=768의 길이를 가진 벡터(flatten)\n",
        "    self.layer_1 = nn.Linear(in_features=self.width*self.height, out_features = self.num_labels)  \n",
        "\n",
        "  def forward(self, input_features, labels=None):\n",
        "    # input_features: [batch, width*height] # batch개의 데이터를 묶어서 학습\n",
        "    # labels: [batch] \n",
        "    # batch를 사용하면 학습속도가 빨라질 뿐 아니라 일반화 능력 상승 (backpropagation)\n",
        "    layer_1_output = self.layer_1(input_features)\n",
        "    # layer_1_output : [batch, num_labels]\n",
        "\n",
        "    activated_output_1 = self.activation(layer_1_output)\n",
        "    # activated_output: [batch, num_labels] \n",
        "\n",
        "    # 학습 시\n",
        "    if labels is not None:\n",
        "      loss_fnc = nn.CrossEntropyLoss() # logit은 2차원, label은 1차원\n",
        "      logits = activated_output_1 # 실제 모델의 출력값\n",
        "      loss = loss_fnc(logits, labels)\n",
        "      return loss\n",
        "    # 평가 시\n",
        "    else:\n",
        "      output = torch.argmax(activated_output_1, -1) # 값이 가장 큰 요소의 index 반환\n",
        "      # output: [batch]\n",
        "      return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEn_PRXdk5eh"
      },
      "source": [
        "<pre>\n",
        "<h2> 1. MNIST 데이터를 읽고 train_X, train_y // test_X, test_y에 저장 </h2>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTQON6GEn0Rg",
        "outputId": "1cfb37ea-8e27-4f12-832d-09c701a2172d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Image(os.path.join(root_dir, \"mnist_example.PNG\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "/content/drive/My Drive/인공지능/week9/mnist_example.PNG",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7WN3AN_R6Q",
        "outputId": "8272919f-479f-4f11-b3d4-ad19e2a36bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import torch\n",
        "def load_dataset():\n",
        "  (train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "  print(train_X.shape) #(60000, 28, 28)\n",
        "  print(train_y.shape)\n",
        "  print(test_X.shape) #(10000, 28, 28)\n",
        "  print(test_y.shape)\n",
        "\n",
        "  train_X = train_X.reshape(-1, 28*28) # 28*28길이의 행을 가지는 array로 변환 \n",
        "  print(train_X.shape)\n",
        "  test_X  = test_X.reshape(-1, 28*28)\n",
        "\n",
        "  # tensor 객체로 변환\n",
        "  train_X = torch.tensor(train_X, dtype=torch.float)\n",
        "  train_y = torch.tensor(train_y, dtype=torch.long)\n",
        "  test_X = torch.tensor(test_X, dtype=torch.float)\n",
        "  test_y = torch.tensor(test_y, dtype=torch.long)\n",
        "\n",
        "  return (train_X, train_y), (test_X, test_y)\n",
        "  \n",
        "tmp = load_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Ngp0TbSwSK"
      },
      "source": [
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkKSHY2Hnln2"
      },
      "source": [
        "<pre>\n",
        "<h2> 2. 불러온 데이터를 이용하여 모델 학습 및 평가 </h2>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE5aJVyfRdn6"
      },
      "source": [
        "# test 데이터로 모델을 평가하는 함수\n",
        "def do_test(model, test_dataloader):\n",
        "  model.eval()\n",
        "  predicts, answers = [], []\n",
        "  for step, batch in enumerate(test_dataloader):\n",
        "    # .cuda()를 통해 메모리에 업로드\n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "    input_features, labels = batch\n",
        "    output = model(input_features)\n",
        "\n",
        "    predicts.extend(tensor2list(output))\n",
        "    answers.extend(tensor2list(labels))\n",
        "    \n",
        "  print(\"Accuracy : {}\".format(accuracy_score(answers, predicts)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7fNFnA_Lhe"
      },
      "source": [
        "def train(config):\n",
        "  # 모델 생성\n",
        "  model = MNIST(config).cuda()\n",
        "\n",
        "  # 데이터 Load\n",
        "  (train_X, train_y), (test_X, test_y) = load_dataset()\n",
        "  \n",
        "  # TensorDataset/DataLoader를 통해 batch 단위로 데이터를 나누고 Shuffle\n",
        "  train_features = TensorDataset(train_X, train_y)\n",
        "  train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "  test_features = TensorDataset(test_X, test_y)\n",
        "  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "  # 모델을 최적화하기 위한 optimizer 선언\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(config[\"epoch\"]):\n",
        "    # epoch 마다 평균 loss를 출력하기위한 loss 저장 리스트\n",
        "    losses = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      # batch = (train_X[step], train_y[step])\n",
        "\n",
        "      # .cuda()를 통해 메모리에 업로드\n",
        "      batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "      # 각 feature 저장\n",
        "      input_features, labels = batch\n",
        "      \n",
        "      # 모델 호출을 통해 loss return\n",
        "      # forward 함수가 호출되는 것 (nn.Module 상속했기 때문)\n",
        "      # labels가 들어갔다 = 학습단계\n",
        "      loss = model(input_features, labels)\n",
        "\n",
        "      # 역전파 변화도 초기화\n",
        "      # ==> .backward() 호출 시, 변화도 버퍼에 데이터가 계속 누적\n",
        "      #     이를 초기화 시켜주는 단계\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "      loss.backward()\n",
        "\n",
        "      # 모델 내부 각 매개변수 가중치 갱신\n",
        "      optimizer.step()\n",
        "\n",
        "      # 1000스텝마다 loss 출력\n",
        "      if (step+1) % 1000 == 0:\n",
        "        print(\"{} step processed.. current loss : {}\".format(step+1, loss.data.item()))\n",
        "      # tensor가 아닌 실제 실수값 loss.data.item() append\n",
        "      losses.append(loss.data.item())\n",
        "    \n",
        "    print(\"Average Loss : {}\".format(np.mean(losses)))\n",
        "    # epoch이 끝날 때 마다, 모델 저장\n",
        "    torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
        "\n",
        "    # 지금까지 학습한 가중치로 평가 진행\n",
        "    do_test(model, test_dataloader)\n",
        "    \n",
        "\n",
        "def test(config):\n",
        "  model = MNIST(config).cuda()\n",
        "\n",
        "  # 저장된 모델 가중치 Load\n",
        "  model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "\n",
        "  # 데이터 load\n",
        "  (_, _), (test_X, test_y) = load_dataset()\n",
        "  \n",
        "  test_features = TensorDataset(test_X, test_y)\n",
        "  test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "  \n",
        "  do_test(model, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1r0OUaUjgnV",
        "outputId": "b25c1dfa-3abd-4ecf-951d-f5590e2c170a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wbpqmro-xhg",
        "outputId": "b553f8f3-260b-4cc9-a3e5-1e3c04c1f0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"input_width_size\":28,\n",
        "              \"input_height_size\":28,\n",
        "              \"feature_size\": 512,\n",
        "              \"num_labels\": 10,\n",
        "              \"batch_size\":32,\n",
        "              \"epoch\":10,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"test\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 784)\n",
            "Accuracy : 0.8231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGiT1Qw_Qzo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}